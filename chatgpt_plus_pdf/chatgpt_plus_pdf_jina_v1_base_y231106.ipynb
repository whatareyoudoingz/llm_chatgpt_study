{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"181BSOH6KF_1o2lFG8DQ6eJd2MZyiSBNt","timestamp":1699237636701},{"file_id":"1a7JJvGUYyzEfAsmHleqlpCay6QlU7Kf9","timestamp":1680680723724}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSBzLXuH1DZb","executionInfo":{"status":"ok","timestamp":1699458165113,"user_tz":-540,"elapsed":1906,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"6564a7df-1479-4f3b-b389-962ffbb30a5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# cd llm_chatgpt_study/chatgpt_plus_pdf/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vpl3vMBZZTim","executionInfo":{"status":"ok","timestamp":1699458168349,"user_tz":-540,"elapsed":308,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"cbf7a8d7-1a88-4ecc-f63f-369847b30e13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/intern/llm_chatgpt_study/chatgpt_plus_pdf\n"]}]},{"cell_type":"markdown","source":["# 질의응답\n","- 참고문헌 : https://www.youtube.com/watch?v=TLf90ipMzfE\n","- colab 자료 : https://colab.research.google.com/drive/181BSOH6KF_1o2lFG8DQ6eJd2MZyiSBNt?usp=sharing"],"metadata":{"id":"buxwP0xxiyol"}},{"cell_type":"markdown","source":["## 필요 모듈 임포트 및 경로지정"],"metadata":{"id":"4j_JAR-xe8yu"}},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AkOXi7saPRIc","executionInfo":{"status":"ok","timestamp":1699458184039,"user_tz":-540,"elapsed":14538,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"e2fd9c2b-387e-4981-d89e-4562887fee04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.0.330 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.0.330)\n","Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.28.1)\n","Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.0.1)\n","Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.7.4)\n","Requirement already satisfied: tiktoken==0.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.5.1)\n","Requirement already satisfied: pypdf==3.17.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.17.0)\n","Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (6.0.1)\n","Requirement already satisfied: SQLAlchemy==2.0.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.22)\n","Requirement already satisfied: aiohttp==3.8.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (3.8.6)\n","Requirement already satisfied: anyio==3.7.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.7.1)\n","Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.0.3)\n","Requirement already satisfied: dataclasses-json==0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.6.1)\n","Requirement already satisfied: jsonpatch==1.33 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.33)\n","Requirement already satisfied: langsmith==0.0.57 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.0.57)\n","Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.23.5)\n","Requirement already satisfied: pydantic==1.10.13 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.10.13)\n","Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.31.0)\n","Requirement already satisfied: tenacity==8.2.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (8.2.3)\n","Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (4.66.1)\n","Requirement already satisfied: regex==2023.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (2023.6.3)\n","Requirement already satisfied: attrs==23.1.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (23.1.0)\n","Requirement already satisfied: charset-normalizer==3.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (3.3.1)\n","Requirement already satisfied: multidict==6.0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (6.0.4)\n","Requirement already satisfied: yarl==1.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.9.2)\n","Requirement already satisfied: frozenlist==1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (1.4.0)\n","Requirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (1.3.1)\n","Requirement already satisfied: idna==3.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (3.4)\n","Requirement already satisfied: sniffio==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (1.3.0)\n","Requirement already satisfied: exceptiongroup==1.1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 29)) (1.1.3)\n","Requirement already satisfied: marshmallow==3.20.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 30)) (3.20.1)\n","Requirement already satisfied: typing-inspect==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 31)) (0.9.0)\n","Requirement already satisfied: jsonpointer==2.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (2.4)\n","Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 33)) (4.5.0)\n","Requirement already satisfied: urllib3==2.0.7 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 34)) (2.0.7)\n","Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (2023.7.22)\n","Requirement already satisfied: greenlet==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 36)) (3.0.0)\n","Requirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 37)) (23.2)\n","Requirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 38)) (1.0.0)\n"]}]},{"cell_type":"code","source":["from PyPDF2 import PdfReader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS"],"metadata":{"id":"nq0vKGFeW1KD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get your API keys from openai, you will need to create an account.\n","# Here is the link to get the keys: https://platform.openai.com/account/billing/overview\n","import os\n","import openai\n","from dotenv import load_dotenv\n","load_dotenv(dotenv_path=\"../../.env.local\") #.env.local에 openai.api_key가 저장되어 있음. 환경변수로 사용함.\n","api_key=os.getenv(\"OPENAI_API_KEY\")\n","openai.api_key = api_key\n","MODEL = \"gpt-3.5-turbo\""],"metadata":{"id":"yKaKB_GjWKjL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 파일 로드"],"metadata":{"id":"If96Xf1zfDKz"}},{"cell_type":"code","source":["# location of the pdf file/files.\n","reader = PdfReader('./data/2023_GPT4All_Technical_Report.pdf')"],"metadata":{"id":"NalD3XkQWrJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwbEBhd0ZUfX","executionInfo":{"status":"ok","timestamp":1699458190884,"user_tz":-540,"elapsed":2,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"92e76af7-3825-4065-cf3a-a61fb95cbd9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PyPDF2._reader.PdfReader at 0x7daf1184d9f0>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## 페이지 속 텍스트 모두 합치기"],"metadata":{"id":"wkJmJuLSfIok"}},{"cell_type":"code","source":["# read data from the file and put them into a variable called raw_text\n","raw_text = ''\n","for i, page in enumerate(reader.pages):\n","    text = page.extract_text()\n","    if text:\n","        raw_text += text"],"metadata":{"id":"2VXlucKiW7bX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## raw_text 분할"],"metadata":{"id":"Gy3UwHGAZa0M"}},{"cell_type":"code","source":["raw_text[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"CQkqUBlzW-Xv","executionInfo":{"status":"ok","timestamp":1699458192772,"user_tz":-540,"elapsed":3,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"7af2a8bd-97ab-4e64-849c-e7e064003318"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nY'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits.\n","\n","text_splitter = CharacterTextSplitter(\n","    separator = \"\\n\", #텍스트 분할 구분자\n","    chunk_size = 1000, #분할된 조각의 크기\n","    chunk_overlap  = 200, #분할된 조각 간 겹치는 부분의 크기\n","    length_function = len, #len을 사용해 길이 측정\n",")\n","texts = text_splitter.split_text(raw_text)"],"metadata":{"id":"VdXzkpf9XAfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ozkNTiNuZ0TX","executionInfo":{"status":"ok","timestamp":1699455850494,"user_tz":-540,"elapsed":4,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"2d1daee4-4018-4e5f-b18e-8235344bffc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["texts[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"1SqdR3wFZ3Ih","executionInfo":{"status":"ok","timestamp":1699243102437,"user_tz":-540,"elapsed":5,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"a022de82-3c50-4a4f-cacc-4abc50d8cc3d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["texts[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"id":"059PoKYUZ6dJ","executionInfo":{"status":"ok","timestamp":1699243103256,"user_tz":-540,"elapsed":3,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"6e62e941-8b26-4ae4-cc05-8106ae6d5b32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'We collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["## 텍스트 데이터 임베딩 및 검색 인덱스 생성"],"metadata":{"id":"m0VPzi2Qi9jR"}},{"cell_type":"code","source":["import sys\n","print(sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNOrIWvhk-Le","executionInfo":{"status":"ok","timestamp":1699458199178,"user_tz":-540,"elapsed":443,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"7a10a181-2281-4e5e-c472-40feedbb258b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"]}]},{"cell_type":"code","source":["# Download embeddings from OpenAI => 텍스트 데이터를 벡터 형식으로 변환\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","embeddings = OpenAIEmbeddings()\n","\n","#검색 인덱스 생성\n","docsearch = FAISS.from_texts(texts, embeddings)#FAISS : 고차원 데이터 검색에 유용, 임베딩된 텍스트 데이터 효율적으로 관리 및 검색 도움\n","docsearch"],"metadata":{"id":"9C8py6wQXE5_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699458206570,"user_tz":-540,"elapsed":3764,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"db2afe24-ceec-454c-8f96-ffbd178b4a52"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain.vectorstores.faiss.FAISS at 0x7daf10ce13f0>"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## 질문응답 체인 로드 및 질문응답 예시 실행"],"metadata":{"id":"MvJIlm9pjH8P"}},{"cell_type":"code","source":["# 질문응답 체인 로드\n","from langchain.chains.question_answering import load_qa_chain\n","from langchain.llms import OpenAI\n","chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"],"metadata":{"id":"d4fllu_Ahn_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"who are the authors of the article?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"3mtAth2jXNKO","executionInfo":{"status":"ok","timestamp":1699243108789,"user_tz":-540,"elapsed":1585,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"f8169d6b-ab1b-4899-b879-2663f166d92a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' The authors of the article are Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["query = \"이 글을 쓴 저자가 누구야?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"zOWsZ-iRyC_5","executionInfo":{"status":"ok","timestamp":1699243135350,"user_tz":-540,"elapsed":1174,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"cbb30e28-f08f-4116-b40b-342bd611263d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["query = \"GPT4all 모델의 가격은 얼마니?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Oinzume4yKdh","executionInfo":{"status":"ok","timestamp":1699243164018,"user_tz":-540,"elapsed":3690,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"3b771062-fc0d-44a2-f090-b095e590b5e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' GPT4all 모델을 훈련하는데 약 8시간의 시간과 Lambda Labs DGX A100 8x 80GB를 사용했을 때 총 비용은 $100입니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["query = \"What was the cost of training the GPT4all model?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"RahXBIXjXO7X","executionInfo":{"status":"ok","timestamp":1699238017157,"user_tz":-540,"elapsed":1843,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"dcd4262b-4246-4694-b0f3-6a62dab35735"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' $100 to train the GPT4all model on a Lambda Labs DGX A100 8x 80GB.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["query = \"How was the model trained?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EzNcvjRJXSZ4","executionInfo":{"status":"ok","timestamp":1699238029473,"user_tz":-540,"elapsed":1545,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"57290948-038c-4a38-9d49-7145e071c7dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' The model was trained with LoRA (Hu et al., 2021) on the 437,605 post-processed examples for four epochs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["query = \"what was the size of the training dataset?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Nhx-kpvAXUl3","executionInfo":{"status":"ok","timestamp":1699238041632,"user_tz":-540,"elapsed":1256,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"e3f6b6b8-3820-473c-cd0f-a32da9432b1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' The final training dataset contained 437,605 prompt-generation pairs.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["query = \"How is this different from other models?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"kIg91Z0YXXCB","executionInfo":{"status":"ok","timestamp":1699238047738,"user_tz":-540,"elapsed":2628,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"bf5c60ba-ed94-464d-ecf1-15e6acb18e08"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" GPT4All is a low-rank adaptation of a large language model, based on LLaMA, and is intended for research purposes only. It is different from other models, such as Stanford Alpaca, because it has a non-commercial license and is based on OpenAI's GPT-3.5-Turbo, which has terms of use that prohibit developing models that compete commercially with OpenAI.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["query = \"What is Google Bard?\"\n","docs = docsearch.similarity_search(query)\n","chain.run(input_documents=docs, question=query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"D02sIID3XagO","executionInfo":{"status":"ok","timestamp":1699238058375,"user_tz":-540,"elapsed":1010,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"0d07eb29-fe00-47dd-f2ba-ae69a6f16a17"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" I don't know.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["# 요약\n","- 참고문헌 : https://teddylee777.github.io/langchain/langchain-tutorial-07/"],"metadata":{"id":"4Rg7XYDbiqvL"}},{"cell_type":"markdown","source":["## 파일 로드 및 분할\n","- 질의응답과 `문서분할`에서 조금 다르다.\n","  - 질의응답은 \"CharacterTextSplitter.split_text(raw_text)\"사용하나, 여기서는 \"CharacterTextSplitter.from_tiktoken_encoder.split_documents()\"를 사용한다."],"metadata":{"id":"_H2Scs2Gj8o6"}},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","loader = PyPDFLoader(\"./data/2023_GPT4All_Technical_Report.pdf\")\n","document = loader.load()\n","document[0].page_content[:200]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"3Xk3dTd-ic1i","executionInfo":{"status":"ok","timestamp":1699458237079,"user_tz":-540,"elapsed":836,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"3d328a73-f683-42d4-e1d6-ed63d05de965"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.a'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","\n","# 스플리터 지정\n","text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n","    separator=\"\\n\\n\",  # 분할기준\n","    chunk_size=3000,   # 사이즈\n","    chunk_overlap=500, # 중첩 사이즈\n",")\n","\n","# 분할 실행\n","split_docs = text_splitter.split_documents(document)\n","\n","# 총 분할된 도큐먼트 수\n","len(split_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aajpKVHbigGN","executionInfo":{"status":"ok","timestamp":1699458241163,"user_tz":-540,"elapsed":2278,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"d2896432-eb2b-4648-b38d-df866c1bb8c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["split_docs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3CUUNWpmwPE","executionInfo":{"status":"ok","timestamp":1699240154342,"user_tz":-540,"elapsed":4,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"a18539af-c265-4847-81b4-d5303ad3c4f2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content='GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-\\nstance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.', metadata={'source': '/content/drive/MyDrive/intern/2023_GPT4All_Technical_Report.pdf', 'page': 0})"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["len(split_docs[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3-hmt_jmmJD","executionInfo":{"status":"ok","timestamp":1699458245770,"user_tz":-540,"elapsed":311,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"d1da6850-14fe-4c73-ca5f-6a8dc710d21d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2794"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## 분할된 각 문서에 대한 요약 실행"],"metadata":{"id":"KUft_krBlk_g"}},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import LLMChain\n","\n","# Map 단계에서 처리할 프롬프트 정의\n","# 분할된 문서에 적용할 프롬프트 내용을 기입합니다.\n","# 여기서 {pages} 변수에는 분할된 문서가 차례대로 대입되니다.\n","map_template = \"\"\"다음은 문서 중 일부 내용입니다\n","{pages}\n","이 문서 목록을 기반으로 주요 내용을 요약해 주세요.\n","답변:\"\"\"\n","\n","# Map 프롬프트 완성\n","map_prompt = PromptTemplate.from_template(map_template)\n","\n","# Map에서 수행할 LLMChain 정의\n","llm = ChatOpenAI(temperature=0,\n","                 model_name=MODEL)\n","map_chain = LLMChain(llm=llm, prompt=map_prompt)"],"metadata":{"id":"H15Jw005lIVH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 각 문서의 요약본에 대한 통합"],"metadata":{"id":"fxHyVV9olKs8"}},{"cell_type":"code","source":["# Reduce 단계에서 처리할 프롬프트 정의\n","reduce_template = \"\"\"다음은 요약의 집합입니다:\n","{doc_summaries}\n","이것들을 바탕으로 통합된 요약을 만들어 주세요.\n","답변:\"\"\"\n","\n","# Reduce 프롬프트 완성\n","reduce_prompt = PromptTemplate.from_template(reduce_template)\n","\n","# Reduce에서 수행할 LLMChain 정의\n","reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"],"metadata":{"id":"CS4T_IFultKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n","from langchain.chains import ReduceDocumentsChain\n","\n","# 문서의 목록을 받아들여, 이를 단일 문자열로 결합하고, 이를 LLMChain에 전달합니다.\n","combine_documents_chain = StuffDocumentsChain(\n","    llm_chain=reduce_chain,\n","    document_variable_name=\"doc_summaries\" # Reduce 프롬프트에 대입되는 변수\n",")\n","\n","# Map 문서를 통합하고 순차적으로 Reduce합니다.\n","reduce_documents_chain = ReduceDocumentsChain(\n","    # 호출되는 최종 체인입니다.\n","    combine_documents_chain=combine_documents_chain,\n","    # 문서가 `StuffDocumentsChain`의 컨텍스트를 초과하는 경우\n","    collapse_documents_chain=combine_documents_chain,\n","    # 문서를 그룹화할 때의 토큰 최대 개수입니다.\n","    token_max=4000,\n",")"],"metadata":{"id":"df019pCvltcb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 통합체인 생성"],"metadata":{"id":"KSzqxGRMl5bN"}},{"cell_type":"code","source":["from langchain.chains import MapReduceDocumentsChain\n","\n","# 문서들에 체인을 매핑하여 결합하고, 그 다음 결과들을 결합합니다.\n","map_reduce_chain = MapReduceDocumentsChain(\n","    # Map 체인\n","    llm_chain=map_chain,\n","    # Reduce 체인\n","    reduce_documents_chain=reduce_documents_chain,\n","    # 문서를 넣을 llm_chain의 변수 이름(map_template 에 정의된 변수명)\n","    document_variable_name=\"pages\",\n","    # 출력에서 매핑 단계의 결과를 반환합니다.\n","    return_intermediate_steps=False,\n",")"],"metadata":{"id":"vcQiwP4rl2__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 요약본 출력"],"metadata":{"id":"wr7UBpkNmzsQ"}},{"cell_type":"code","source":["split_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVBkt65bm3jO","executionInfo":{"status":"ok","timestamp":1699458255042,"user_tz":-540,"elapsed":7,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"fc541f5d-f448-4154-c9a5-e5678012182f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-\\nset from the final training dataset due to its very\\nFigure 1: TSNE visualization of the candidate training\\ndata (Red: Stackoverflow, Orange: chip2, Blue: P3).\\nThe large blue balls (e.g. indicated by the red arrow)\\nare highly homogeneous prompt-response pairs.\\nlow output diversity; P3 contains many homoge-\\nneous prompts which produce short and homoge-\\nneous responses from GPT-3.5-Turbo. This exclu-\\nsion produces a final subset containing 437,605\\nprompt-generation pairs, which is visualized in\\nFigure 2. You can interactively explore the dataset\\nat each stage of cleaning at the following links:\\n• Cleaned with P3\\n• Cleaned without P3 (Final Training Dataset)\\n2 Model Training\\nWe train several models finetuned from an in-\\nstance of LLaMA 7B (Touvron et al., 2023).\\nThe model associated with our initial public re-\\nlease is trained with LoRA (Hu et al., 2021)\\non the 437,605 post-processed examples for four\\nepochs. Detailed model hyper-parameters and\\ntraining code can be found in the associated repos-\\nitory and model training log.', metadata={'source': './data/2023_GPT4All_Technical_Report.pdf', 'page': 0}),\n"," Document(page_content='(a) TSNE visualization of the final training data, ten-colored\\nby extracted topic.\\n(b) Zoomed in view of Figure 2a. The region displayed con-\\ntains generations related to personal health and wellness.\\nFigure 2: The final training data was curated to ensure a diverse distribution of prompt topics and model responses.\\n2.1 Reproducibility\\nWe release all data (including unused P3 genera-\\ntions), training code, and model weights for the\\ncommunity to build upon. Please check the Git\\nrepository for the most up-to-date data, training\\ndetails and checkpoints.\\n2.2 Costs\\nWe were able to produce these models with about\\nfour days work, $800 in GPU costs (rented from\\nLambda Labs and Paperspace) including several\\nfailed trains, and $500 in OpenAI API spend.\\nOur released model, gpt4all-lora, can be trained in\\nabout eight hours on a Lambda Labs DGX A100\\n8x 80GB for a total cost of $100 .\\n3 Evaluation\\nWe perform a preliminary evaluation of our model\\nusing the human evaluation data from the Self-\\nInstruct paper (Wang et al., 2022). We report the\\nground truth perplexity of our model against what\\nis, to our knowledge, the best openly available\\nalpaca-lora model, provided by user chainyo on\\nhuggingface. We find that all models have very\\nlarge perplexities on a small number of tasks, and\\nreport perplexities clipped to a maximum of 100.\\nModels finetuned on this collected dataset ex-\\nhibit much lower perplexity in the Self-Instruct\\nevaluation compared to Alpaca. This evaluation is\\nin no way exhaustive and further evaluation work\\nFigure 3: Model Perplexities. Lower is better. Our\\nmodels achieve stochastically lower ground truth per-\\nplexities than alpaca-lora.\\nremains. We welcome the reader to run the model\\nlocally on CPU (see Github for files) and get a\\nqualitative sense of what it can do.\\n4 Use Considerations\\nThe authors release data and training details in\\nhopes that it will accelerate open LLM research,\\nparticularly in the domains of alignment and inter-\\npretability. GPT4All model weights and data are\\nintended and licensed only for research purposes\\nand any commercial use is prohibited. GPT4All\\nis based on LLaMA, which has a non-commercial\\nlicense. The assistant data is gathered from Ope-\\nnAI’s GPT-3.5-Turbo, whose terms of use pro-', metadata={'source': './data/2023_GPT4All_Technical_Report.pdf', 'page': 1}),\n"," Document(page_content='hibit developing models that compete commer-\\ncially with OpenAI.\\nReferences\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\\nlarge language models.\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\\nDubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\\nford alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/\\nstanford_alpaca .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. 2023.\\nLlama: Open and efficient foundation language\\nmodels.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\\nguage model with self generated instructions.', metadata={'source': './data/2023_GPT4All_Technical_Report.pdf', 'page': 2})]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# Map-Reduce 체인 실행\n","# 입력: 분할된 도큐먼트\n","result = map_reduce_chain.run(split_docs)\n","# 요약결과 출력\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_IlDhbMl66Q","executionInfo":{"status":"ok","timestamp":1699458294263,"user_tz":-540,"elapsed":37425,"user":{"displayName":"jina kim","userId":"01101908893114905171"}},"outputId":"e1bf7cf2-0868-42b9-a5bb-dda2bd338f57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPT4All은 대규모 데이터를 사용하여 훈련된 챗봇으로, 다양한 상호작용을 포함한 데이터를 사용하여 훈련되었습니다. 데이터 수집 및 정제 과정에서는 GPT-3.5-Turbo OpenAI API를 사용하여 프롬프트-응답 쌍을 수집하고, 훈련에서는 여러 모델을 사용하여 파인튜닝을 진행했습니다. 이 문서는 GPT4All 모델에 대한 정보와 재현성, 비용, 평가, 사용 고려 사항에 대한 내용을 제공합니다. 또한, 다양한 언어 모델에 대한 참조도 제공됩니다.\n"]}]},{"cell_type":"code","source":["#"],"metadata":{"id":"aFILdF2sl815"},"execution_count":null,"outputs":[]}]}